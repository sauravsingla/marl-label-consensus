{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f71870",
   "metadata": {},
   "source": [
    "# MARL Collaborative Label Aggregation Demo\n",
    "Train multiple agents to reach consensus on noisy labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30d0989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.env.label_env import LabelAggregationEnv\n",
    "from src.agents.dqn import DQNAgent\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c406ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and agents\n",
    "env = LabelAggregationEnv(num_agents=3)\n",
    "agents = [DQNAgent(input_dim=1, output_dim=2) for _ in range(3)]\n",
    "\n",
    "rewards_log = [[] for _ in agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5aaa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "state = env.reset()\n",
    "for step in range(100):\n",
    "    actions = [agent.act(state) for agent in agents]\n",
    "    next_state, rewards, done, _ = env.step(actions)\n",
    "    for i, agent in enumerate(agents):\n",
    "        agent.remember(state, actions[i], rewards[i], next_state)\n",
    "        agent.learn()\n",
    "        rewards_log[i].append(rewards[i])\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards\n",
    "for i, r in enumerate(rewards_log):\n",
    "    plt.plot(r, label=f\"Agent {i}\")\n",
    "plt.title(\"Agent Rewards Over Time\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
